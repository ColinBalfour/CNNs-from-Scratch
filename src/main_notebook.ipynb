{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593eb79b",
   "metadata": {},
   "source": [
    "# P1: Nifty Neural Networks!\n",
    "\n",
    "\n",
    "## Table Of Content\n",
    "\n",
    "1. Introduction\n",
    "2. Preliminaries\n",
    "3. Software Setup\n",
    "4. Implementation\n",
    "5. Grading Rubric\n",
    "6. Report guidelines\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Neural networks, at their core, function like any other mathematical function that can be evaluated. The process of evaluating a neural network is referred to as the forward pass. During this step, inputs are passed through the network layers, and outputs are generated.\n",
    "\n",
    "To optimize the network's performance, its weights and biases need to be adjusted. This is done through a process called backward propagation (or backpropagation). In this step, the gradients of the loss function with respect to each parameter are calculated, and these gradients are subtracted from the corresponding weights and biases, allowing the network to learn and improve its predictions.\n",
    "\n",
    "In this assignment, you will dive into the implementation of custom layers in PyTorch. Specifically, you will focus on coding the forward pass and computing the gradients necessary for the backward pass. Before you begin, make sure to review the grading rubric to understand the criteria for evaluation.\n",
    "\n",
    "## 2. Preliminaries\n",
    "\n",
    "### CIFAR10 Dataset\n",
    "\n",
    "CIFAR-10 is a dataset consisting of 60000, 32Ã—32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More details about the datset can be found [here](http://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Sample images from each class of the CIFAR-10 dataset is shown below:\n",
    "\n",
    "![CIFAR 10](./artifacts/cifar10.png)\n",
    "\n",
    "In this project, you will classify images into these 10 classes using the provided pipeline,loaders and helper classes.\n",
    "\n",
    "Additionally, you are expected to generate a confusion matrix to evaluate your model's performance. For guidance on plotting a confusion matrix in PyTorch, please refer to this [resource](https://stackoverflow.com/questions/74020233/how-to-plot-confusion-matrix-in-pytorch).\n",
    "\n",
    "### Linear Layer\n",
    "A linear layer in a neural network performs a linear transformation of the input data. It is defined by the following components:\n",
    "\n",
    "1. Weights\n",
    "2. Biases\n",
    "\n",
    "More details below,\n",
    "\n",
    "[Pytorch Linear Layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "You can find information about the dimension of weights and biases in custom_layers.py\n",
    "\n",
    "### Soft Max\n",
    "The Softmax function is commonly used in neural networks for multi-class classification problems. It converts a vector of raw scores (logits) into probabilities, making it possible to interpret the output as the likelihood of each class.\n",
    "\n",
    "[Sample implementation](https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python)\n",
    "\n",
    "More details [here](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "A convolutional layer is a fundamental building block in Convolutional Neural Networks (CNNs) used primarily for processing grid-like data such as images. It applies convolution operations to detect local features in the input.\n",
    "\n",
    "Although it is called a convolutional layer, the PyTorch implementation of conv2d does not actually perform a convolution in the mathematical sense. Instead, it performs a cross-correlation operation, where the kernel is not flipped. This distinction is important to note, but for most deep learning projects including this one, cross-correlation is perfectly fine as the weights will automatically adjust during training.\n",
    "\n",
    "For more details, refer to [P0](https://rbe549.github.io/rbe474x/fall2024a/proj/p0/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f305e",
   "metadata": {},
   "source": [
    "## 3. Software Setup\n",
    "\n",
    "Use a code editor like VSCode and open this entire folder.\n",
    "\n",
    "For each part, you will be implementing the corresponding layers in custom_layers.py\n",
    "\n",
    "The code will automatically be tested with test.py. \n",
    "\n",
    "To run the test, open a terminal in the current folder and run,\n",
    "\n",
    "`pytest -s -v test.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c586079",
   "metadata": {},
   "source": [
    "## 4. Implementation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925fff4",
   "metadata": {},
   "source": [
    "### Part1 : Implement Your Custom Layers for Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb96b9",
   "metadata": {},
   "source": [
    "Open custom_layers.py and implement a fully connected, relu and softmax layer.\n",
    "\n",
    "Verify it by running the below code. Feel free to modify the below snippet. But do not modify my test.py\n",
    "\n",
    "For more information about supplying gradients, please refer to [examples_autograd](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc90238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear\n",
      "Inference for linear layer\n",
      "tensor([[-0.1176, -0.2610]], device='cuda:0',\n",
      "       grad_fn=<CustomLinearLayerBackward>)\n",
      "tensor([[-0.1176, -0.2610]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "\n",
      "graidents for linear layer\n",
      "tensor([[-0.0054, -0.0812, -0.0046, -0.0361, -0.1074, -0.0557, -0.1023, -0.0170,\n",
      "         -0.1075, -0.0299],\n",
      "        [-0.0121, -0.1802, -0.0102, -0.0802, -0.2386, -0.1237, -0.2271, -0.0377,\n",
      "         -0.2388, -0.0664]], device='cuda:0')\n",
      "tensor([[-0.0054, -0.0812, -0.0046, -0.0361, -0.1074, -0.0557, -0.1023, -0.0170,\n",
      "         -0.1075, -0.0299],\n",
      "        [-0.0121, -0.1802, -0.0102, -0.0802, -0.2386, -0.1237, -0.2271, -0.0377,\n",
      "         -0.2388, -0.0664]], device='cuda:0')\n",
      "tensor([-0.1176, -0.2610], device='cuda:0')\n",
      "tensor([-0.1176, -0.2610], device='cuda:0')\n",
      "\n",
      "RELU\n",
      "inference\n",
      "tensor([[0.8006, 0.7644, 0.8570, 0.5406, 0.5683, 0.6986, 0.0423, 0.8489, 0.9402,\n",
      "         0.1005]], grad_fn=<CustomReLULayerBackward>)\n",
      "tensor([[0.8006, 0.7644, 0.8570, 0.5406, 0.5683, 0.6986, 0.0423, 0.8489, 0.9402,\n",
      "         0.1005]], grad_fn=<ReluBackward0>)\n",
      "gradients of loss relative to the input\n",
      "tensor([[0.1601, 0.1529, 0.1714, 0.1081, 0.1137, 0.1397, 0.0085, 0.1698, 0.1880,\n",
      "         0.0201]])\n",
      "tensor([[0.1601, 0.1529, 0.1714, 0.1081, 0.1137, 0.1397, 0.0085, 0.1698, 0.1880,\n",
      "         0.0201]])\n",
      "\n",
      " SoftMax\n",
      "gradients of loss relative to the input\n",
      "tensor([[-0.0208, -0.0165,  0.0373]])\n",
      "tensor([[-0.0208, -0.0165,  0.0373]])\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import networks as net\n",
    "importlib.reload(net)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"\\nLinear\")\n",
    "u = torch.rand((1, 10)).to(device)\n",
    "customLayer = net.CustomLinear(10, 2).to(device)\n",
    "inbuiltLayer = nn.Linear(in_features=10, out_features=2).to(device)\n",
    "\n",
    "inbuiltLayer.weight.data.copy_(customLayer.weight.data)\n",
    "inbuiltLayer.bias.data.copy_(customLayer.bias.data)\n",
    "\n",
    "y_custom = customLayer(u)\n",
    "y_inbuilt = inbuiltLayer(u)\n",
    "print(\"Inference for linear layer\")\n",
    "print(y_custom)\n",
    "print(y_inbuilt)\n",
    "\n",
    "lossFunc = nn.MSELoss()\n",
    "\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "loss_in.backward()\n",
    "\n",
    "loss_custom = lossFunc(\n",
    "    y_custom,\n",
    "    torch.zeros_like(\n",
    "        y_custom,\n",
    "    ),\n",
    ")\n",
    "loss_custom.backward()\n",
    "\n",
    "\n",
    "print(\"\\ngraidents for linear layer\")\n",
    "print(customLayer.weight.grad)\n",
    "print(inbuiltLayer.weight.grad)\n",
    "\n",
    "print(customLayer.bias.grad)\n",
    "print(inbuiltLayer.bias.grad)\n",
    "\n",
    "# RELU\n",
    "print(\"\\nRELU\")\n",
    "u1 = torch.rand((1, 10), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "\n",
    "customLayer = net.CustomReLU()\n",
    "inbuiltLayer = nn.ReLU()\n",
    "\n",
    "y_custom = customLayer(u1)\n",
    "y_inbuilt = inbuiltLayer(u2)\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"inference\")\n",
    "print(y_custom)\n",
    "print(y_inbuilt)\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n",
    "\n",
    "# SOFTMAX\n",
    "print(\"\\n SoftMax\")\n",
    "\n",
    "u1 = torch.rand((1, 3), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "customLayer = net.CustomSoftmax(1)\n",
    "inbuiltLayer = nn.Softmax()\n",
    "\n",
    "y_custom = customLayer(u1)\n",
    "y_inbuilt = inbuiltLayer(u2)\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0ae69",
   "metadata": {},
   "source": [
    "### Part 2: MLP Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10d7c7",
   "metadata": {},
   "source": [
    "Now that you have implemented an MLP from scratch, it's time to train it and verify its ability to classify objects. This network is expected to achieve an accuracy of approximately 40%.\n",
    "\n",
    "Additionally, you are required to save one of your best model checkpoints as mlp.pth in the current folder. This file will be used for automated testing.\n",
    "\n",
    "Furthermore, please implement a confusion matrix in the utils file, specifically within the val_step method of the Pipeline class. You may use any available implementation of the confusion matrix, but ensure that all tests continue to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8da3018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(net)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m---> 11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mCustomMLP()\u001b[38;5;241m.\u001b[39mto(pipeline\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/Group5_p1/src/utils.py:18\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m     14\u001b[0m     [transforms\u001b[38;5;241m.\u001b[39mGrayscale(num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Convert image to grayscale\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     16\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))])  \u001b[38;5;66;03m# Normalize for grayscale images\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m testset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:66\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:140\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/.venv/lib/python3.10/site-packages/torchvision/datasets/utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 395\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/.venv/lib/python3.10/site-packages/torchvision/datasets/utils.py:132\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[0;32m--> 132\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/.venv/lib/python3.10/site-packages/torchvision/datasets/utils.py:32\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m response\u001b[38;5;241m.\u001b[39mread(chunk_size):\n\u001b[1;32m     31\u001b[0m     fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/.venv/lib/python3.10/site-packages/torch/hub.py:41\u001b[0m, in \u001b[0;36m_Faketqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codeing/junior/rbe474x/.venv/lib/python3.10/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "pipeline = net.Pipeline()\n",
    "\n",
    "model = net.CustomMLP().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/mlp/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "for eIndex in range(epochs):\n",
    "    print(\"Epoch count: \", eIndex)\n",
    "\n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train.log\", trainLosses)\n",
    "np.savetxt(\"test.log\", testAccuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b1e2d",
   "metadata": {},
   "source": [
    "### Part 3: Implement Convolutional Neural Networks (CNN) Using PyTorch layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c6b38",
   "metadata": {},
   "source": [
    "CNNs excel in capturing local patterns and spatial hierarchies through convolutional filters, which makes them more effective for image and spatial data. They also use parameter sharing, reducing the number of parameters and computational cost compared to MLPs. Additionally, CNNs offer translation invariance and hierarchical feature learning, enabling them to recognize features across different spatial locations and build complex patterns efficiently.\n",
    "\n",
    "Open networks.py and implement `RefCNN` using the inbuilt layers in pytorch. Make sure it is similar to CustomCNN() which uses custom layers.\n",
    "\n",
    "Train and compare the train loss and validation accuracy against MLP. \n",
    "\n",
    "Please copy the best checkpoint file in current folder as cnn_inbuilt.pth for automated tests. It is expected to be higher than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c9a9cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mPipeline()\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mRefCNN()\u001b[38;5;241m.\u001b[39mto(pipeline\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m home_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m JOB_FOLDER\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(home_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:72\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     61\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     62\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:362\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    360\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    364\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "pipeline = net.Pipeline()\n",
    "model = net.RefCNN().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/cnn_inbuilt_layers/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "for eIndex in range(epochs):\n",
    "    # print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    print(\"train complete\")\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train.log\", trainLosses)\n",
    "np.savetxt(\"test.log\", testAccuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b9224",
   "metadata": {},
   "source": [
    "### Part 4: Implement Your Custom Layers for Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e78b4fe",
   "metadata": {},
   "source": [
    "Open custom_layers.py and implement the CustomConvLayer.\n",
    "\n",
    "Verify it by running the below code. Feel free to modify the below snippet. But do not modify my test.py\n",
    "\n",
    "For more information about supplying gradients, please refer to [examples_autograd](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "inbuiltLayer = nn.Conv2d(2, 3, 3, stride=2, padding='valid')\n",
    "customLayer = net.CustomConv2d(2, 3, 3, 2)\n",
    "\n",
    "inbuiltLayer.weight.data.copy_(customLayer.weight.data)\n",
    "inbuiltLayer.bias.data.copy_(customLayer.bias.data)\n",
    "\n",
    "u1 = torch.rand((1, 2, 5, 5), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "\n",
    "y1 = inbuiltLayer(u1)\n",
    "y2 = customLayer(u2)\n",
    "\n",
    "print(\"Conv. Inference\")\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "lossFunc = nn.MSELoss()\n",
    "loss_custom = lossFunc(y2, torch.zeros_like(y2))\n",
    "loss_in = lossFunc(y1, torch.zeros_like(y1))\n",
    "\n",
    "loss_in.backward()\n",
    "loss_custom.backward()\n",
    "\n",
    "print(\"gradients of loss relative to the weights\")\n",
    "print(inbuiltLayer.weight.grad)\n",
    "print(customLayer.weight.grad)\n",
    "\n",
    "print(\"gradients of loss relative to the bias\")\n",
    "print(inbuiltLayer.bias.grad)\n",
    "print(customLayer.bias.grad)\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf028b78",
   "metadata": {},
   "source": [
    "### Part 5: CNN Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ca731",
   "metadata": {},
   "source": [
    "Train and compare the train loss and validation accuracy against MLP and inbuilt conv layers. \n",
    "\n",
    "Please copy the best checkpoint file in current folder as `cnn_custom.pth` for automated tests. It is expected to be higher than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f31b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "pipeline = net.Pipeline()\n",
    "model = net.CustomCNN().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/cnn_custom_layer/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "for eIndex in range(epochs):\n",
    "    # print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    print(\"train complete\")\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train.log\", trainLosses)\n",
    "np.savetxt(\"test.log\", testAccuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54272187",
   "metadata": {},
   "source": [
    "## 5. Grading Rubric\n",
    "\n",
    "- part 1 : 60\n",
    "- part 2 : 10\n",
    "- part 3 : 10\n",
    "- part 4 : 10\n",
    "- part 5 : 10\n",
    "\n",
    "For RBE474X: part1 + part2 + part3 = 100% of the grade (80/80).\n",
    "For RBE595-A01-SP: You are expected to implement part1-part5 for getting full credits (100/100).\n",
    "\n",
    "Your code will be evaluated with test.py. Please run it and ensure that the tests pass before submitting. Instructions are in software setup section.\n",
    "\n",
    "Please note that I will replace the test.py with my original test.py before evaluating.\n",
    "\n",
    "Please do not submit the data folder that is downloaded while training the network. It is over 300 MB. Anyone submitting data will be penalized! Your submission should not be more than 20 MB.\n",
    "\n",
    "## 6. Report Guidelines\n",
    "\n",
    "Report must be in Latex.\n",
    "\n",
    "Include the following,\n",
    "\n",
    "1. Training loss curve (loss vs epoch count)\n",
    "2. Confusion Matrix for validation set (val_step)\n",
    "3. Accuracy comparison between MLP, CNN (torch layers) and CNN (custom_layers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
